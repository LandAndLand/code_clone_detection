{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from code_parser import *\n",
    "from dgl_dataset import CloneDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from code_parser import *\n",
    "from dgl_dataset import CloneDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graph1, graph2, labels = map(list, zip(*samples))\n",
    "    batched_graph1 = dgl.batch(graph1)\n",
    "    batched_graph2 = dgl.batch(graph2)\n",
    "    return batched_graph1, batched_graph2, torch.tensor(labels)\n",
    "\n",
    "\n",
    "class NetBasic(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(NetBasic, self).__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.conv1 = GraphConv(self.hparams.num_features, self.hparams.hidden_dim)\n",
    "        self.conv2 = GraphConv(self.hparams.hidden_dim, self.hparams.hidden_dim)\n",
    "        self.conv3 = GraphConv(self.hparams.hidden_dim, self.hparams.hidden_dim)\n",
    "        self.classify = nn.Linear(self.hparams.hidden_dim * 2, self.hparams.num_classes)\n",
    "\n",
    "    def forward_(self, g1, g2):\n",
    "        h1 = g1.ndata['data'].view(-1, self.hparams.num_features).float().to(device)\n",
    "        h1 = F.relu(self.conv1(g1, h1))\n",
    "        h1 = F.relu(self.conv2(g1, h1))\n",
    "#         h1 = F.relu(self.conv3(g1, h1))\n",
    "        g1.ndata['h'] = h1\n",
    "\n",
    "        h2 = g2.ndata['data'].view(-1, self.hparams.num_features).float().to(device)\n",
    "        h2 = F.relu(self.conv1(g2, h2))\n",
    "        h2 = F.relu(self.conv2(g2, h2))\n",
    "#         h2 = F.relu(self.conv3(g2, h2))\n",
    "        g2.ndata['h'] = h2\n",
    "\n",
    "        hg1 = dgl.mean_nodes(g1, 'h')\n",
    "        hg2 = dgl.mean_nodes(g2, 'h')\n",
    "\n",
    "        return F.log_softmax(self.classify(torch.cat([hg1, hg2], dim=-1)), dim=-1)\n",
    "\n",
    "    def forward(self, g1, g2):\n",
    "        return self.forward_(g1, g2)\n",
    "\n",
    "    def training_step(self, data):\n",
    "        g1, g2, label = data\n",
    "        output = self.forward(g1, g2)\n",
    "        loss = F.nll_loss(output, label.to(device))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, data):\n",
    "        g1, g2, label = data\n",
    "        output = self.forward(g1, g2)\n",
    "        loss = F.nll_loss(output, label.to(device))\n",
    "        pred = output.max(dim=1)[1]\n",
    "        acc = pred.eq(label.cuda()).type(torch.float32).mean()\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_val_loss': avg_loss, 'avg_val_acc': avg_acc}\n",
    "        return {'val_loss': avg_loss, 'val_acc': avg_acc, 'log': tensorboard_logs}\n",
    "\n",
    "#         def test_step(self, data, batch_idx):\n",
    "#             g1, g2, label = data\n",
    "#             output = self.forward(g1, g2)\n",
    "#             loss = F.cross_entropy(output, label)\n",
    "#             pred = torch.softmax(output, 1).max(dim=1)[1]\n",
    "#             acc = pred.eq(data.y).type(torch.float32).mean()\n",
    "#             return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "#         def test_epoch_end(self, outputs):\n",
    "#             avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#             avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "\n",
    "#             tensorboard_logs = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_acc}\n",
    "#             return {'test_loss': avg_loss, 'test_acc': avg_acc, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = CloneDataset(\n",
    "            functions_path=os.path.join(self.hparams.root, \"dgl_functions\"),\n",
    "            pairs_path=os.path.join(self.hparams.root, \"bcb_pair_ids.pkl\"),\n",
    "        )\n",
    "\n",
    "        n = len(dataset)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = dgl.data.utils.split_dataset(dataset, frac_list =[0.6, 0.15, 0.25], shuffle=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED.\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.workers,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.workers,\n",
    "                          collate_fn=collate)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(self.test_dataset,\n",
    "                              batch_size=self.hparams.batch_size,\n",
    "                              num_workers=self.hparams.workers,\n",
    "                              collate_fn=collate)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args():\n",
    "        parser = ArgumentParser(add_help=False)\n",
    "\n",
    "        parser.add_argument('--learning_rate', default=0.0001, type=float)\n",
    "        parser.add_argument('--batch_size', default=32, type=int)\n",
    "        parser.add_argument('--workers', default='8', type=int)\n",
    "        parser.add_argument('--num_classes', default='6', type=int)\n",
    "        parser.add_argument('--num_features', default='384', type=int)\n",
    "        parser.add_argument('--hidden_dim', default='284', type=int)\n",
    "\n",
    "        parser.add_argument('--root', type=str, required=True)\n",
    "\n",
    "        # training specific (for this model)\n",
    "        parser.add_argument('--gpus', type=int, default=1, help='how many gpus')\n",
    "\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=64,\n",
    "    workers=64,\n",
    "    num_classes=6,\n",
    "    num_features=384,\n",
    "    hidden_dim=284,\n",
    "    gpu=1,\n",
    "    root=\"../data/\",\n",
    "    max_nb_epochs=2\n",
    ")\n",
    "from argparse import Namespace\n",
    "\n",
    "hparams = Namespace(**params)\n",
    "model = NetBasic(hparams).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.prepare_data()\n",
    "train_loader = model.train_dataloader()\n",
    "val_loader = model.val_dataloader()\n",
    "\n",
    "optimizer, scheduler = model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6320367128952691\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logs = model.training_step(data)\n",
    "        loss = logs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"loss = {np.mean(losses)}\", end=\"\\r\")\n",
    "    \n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for data in val_loader:\n",
    "        log = model.validation_step(data)\n",
    "        outputs.append(log)\n",
    "    logs = model.validation_epoch_end(outputs)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}; loss: {np.mean(losses)}; val_loss: {logs['val_loss']}; val_acc: {logs['val_acc']}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    torch.save(model.state_dict(), \"../data/dgl_play.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
