{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim.models import KeyedVectors as word2vec\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "\n",
    "from code_parser import *\n",
    "from dataset import CloneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.NormalizeFeatures()\n",
    "dataset = CloneDataset(root=\"../data/\", functions_path=\"../data/functions/\", pairs_path=\"../data/bcb_pair_ids.pkl\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, num_workers=64)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(dataset.num_features, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        print(f\"loss = {loss.item()}\", end=\"\\r\")\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.57298, Train Acc: 0.80562, Val Acc: 0.80599, Best: 0.80599\n",
      "Epoch: 014, Loss: 0.56581, Train Acc: 0.80845, Val Acc: 0.80845, Best: 0.80845\n",
      "Epoch: 015, Loss: 0.55768, Train Acc: 0.81091, Val Acc: 0.81163, Best: 0.81163\n",
      "Epoch: 016, Loss: 0.55246, Train Acc: 0.81339, Val Acc: 0.81224, Best: 0.81224\n",
      "Epoch: 017, Loss: 0.54628, Train Acc: 0.81638, Val Acc: 0.81583, Best: 0.81583\n",
      "Epoch: 018, Loss: 0.53971, Train Acc: 0.81883, Val Acc: 0.81819, Best: 0.81819\n",
      "Epoch: 019, Loss: 0.53611, Train Acc: 0.82092, Val Acc: 0.81840, Best: 0.81840\n",
      "Epoch: 020, Loss: 0.53071, Train Acc: 0.82342, Val Acc: 0.82055, Best: 0.82055\n",
      "Epoch: 021, Loss: 0.52401, Train Acc: 0.82277, Val Acc: 0.82075, Best: 0.82075\n",
      "Epoch: 022, Loss: 0.52307, Train Acc: 0.82626, Val Acc: 0.82424, Best: 0.82424\n",
      "Epoch: 023, Loss: 0.51710, Train Acc: 0.82832, Val Acc: 0.82506, Best: 0.82506\n",
      "Epoch: 024, Loss: 0.51357, Train Acc: 0.83047, Val Acc: 0.82475, Best: 0.82506\n",
      "Epoch: 025, Loss: 0.50977, Train Acc: 0.83207, Val Acc: 0.82578, Best: 0.82578\n",
      "Epoch: 026, Loss: 0.50584, Train Acc: 0.83216, Val Acc: 0.82404, Best: 0.82578\n",
      "Epoch: 027, Loss: 0.50285, Train Acc: 0.83504, Val Acc: 0.82804, Best: 0.82804\n",
      "Epoch: 028, Loss: 0.50057, Train Acc: 0.83674, Val Acc: 0.83029, Best: 0.83029\n",
      "Epoch: 029, Loss: 0.49668, Train Acc: 0.83774, Val Acc: 0.83132, Best: 0.83132\n",
      "Epoch: 030, Loss: 0.49458, Train Acc: 0.83928, Val Acc: 0.83337, Best: 0.83337\n",
      "Epoch: 031, Loss: 0.49189, Train Acc: 0.83907, Val Acc: 0.83091, Best: 0.83337\n",
      "Epoch: 032, Loss: 0.48920, Train Acc: 0.84043, Val Acc: 0.83203, Best: 0.83337\n",
      "Epoch: 033, Loss: 0.48458, Train Acc: 0.84352, Val Acc: 0.83378, Best: 0.83378\n",
      "Epoch: 034, Loss: 0.48185, Train Acc: 0.84265, Val Acc: 0.83429, Best: 0.83429\n",
      "Epoch: 035, Loss: 0.47953, Train Acc: 0.84492, Val Acc: 0.83767, Best: 0.83767\n",
      "Epoch: 036, Loss: 0.47965, Train Acc: 0.84538, Val Acc: 0.83778, Best: 0.83778\n",
      "Epoch: 037, Loss: 0.47545, Train Acc: 0.84602, Val Acc: 0.83778, Best: 0.83778\n",
      "Epoch: 038, Loss: 0.47352, Train Acc: 0.84679, Val Acc: 0.83839, Best: 0.83839\n",
      "Epoch: 039, Loss: 0.47038, Train Acc: 0.84655, Val Acc: 0.83911, Best: 0.83911\n",
      "Epoch: 040, Loss: 0.46773, Train Acc: 0.84784, Val Acc: 0.84024, Best: 0.84024\n",
      "Epoch: 041, Loss: 0.46530, Train Acc: 0.85026, Val Acc: 0.83901, Best: 0.84024\n",
      "Epoch: 042, Loss: 0.46513, Train Acc: 0.85021, Val Acc: 0.84055, Best: 0.84055\n",
      "Epoch: 043, Loss: 0.46085, Train Acc: 0.85071, Val Acc: 0.84116, Best: 0.84116\n",
      "Epoch: 044, Loss: 0.46145, Train Acc: 0.85135, Val Acc: 0.84331, Best: 0.84331\n",
      "Epoch: 045, Loss: 0.46038, Train Acc: 0.85183, Val Acc: 0.84198, Best: 0.84331\n",
      "Epoch: 046, Loss: 0.45750, Train Acc: 0.85276, Val Acc: 0.84413, Best: 0.84413\n",
      "Epoch: 047, Loss: 0.45426, Train Acc: 0.85426, Val Acc: 0.84342, Best: 0.84413\n",
      "Epoch: 048, Loss: 0.45329, Train Acc: 0.85273, Val Acc: 0.84137, Best: 0.84413\n",
      "Epoch: 049, Loss: 0.45279, Train Acc: 0.85512, Val Acc: 0.84383, Best: 0.84413\n",
      "Epoch: 050, Loss: 0.44985, Train Acc: 0.85565, Val Acc: 0.84526, Best: 0.84526\n",
      "Epoch: 051, Loss: 0.44780, Train Acc: 0.85596, Val Acc: 0.84311, Best: 0.84526\n",
      "Epoch: 052, Loss: 0.44632, Train Acc: 0.85720, Val Acc: 0.84557, Best: 0.84557\n",
      "Epoch: 053, Loss: 0.44510, Train Acc: 0.85747, Val Acc: 0.84506, Best: 0.84557\n",
      "Epoch: 054, Loss: 0.44218, Train Acc: 0.85802, Val Acc: 0.84454, Best: 0.84557\n",
      "Epoch: 055, Loss: 0.44075, Train Acc: 0.85735, Val Acc: 0.84331, Best: 0.84557\n",
      "Epoch: 056, Loss: 0.43926, Train Acc: 0.85808, Val Acc: 0.84598, Best: 0.84598\n",
      "Epoch: 057, Loss: 0.43851, Train Acc: 0.85966, Val Acc: 0.84649, Best: 0.84649\n",
      "Epoch: 058, Loss: 0.43756, Train Acc: 0.85757, Val Acc: 0.84444, Best: 0.84649\n",
      "Epoch: 059, Loss: 0.43728, Train Acc: 0.86103, Val Acc: 0.84885, Best: 0.84885\n",
      "Epoch: 060, Loss: 0.43349, Train Acc: 0.86269, Val Acc: 0.84906, Best: 0.84906\n",
      "Epoch: 061, Loss: 0.43370, Train Acc: 0.85784, Val Acc: 0.84372, Best: 0.84906\n",
      "Epoch: 062, Loss: 0.43201, Train Acc: 0.86180, Val Acc: 0.84731, Best: 0.84906\n",
      "Epoch: 063, Loss: 0.43011, Train Acc: 0.86319, Val Acc: 0.84629, Best: 0.84906\n",
      "Epoch: 064, Loss: 0.42922, Train Acc: 0.86361, Val Acc: 0.84731, Best: 0.84906\n",
      "Epoch: 065, Loss: 0.42587, Train Acc: 0.86392, Val Acc: 0.84865, Best: 0.84906\n",
      "Epoch: 066, Loss: 0.42622, Train Acc: 0.86442, Val Acc: 0.84895, Best: 0.84906\n",
      "Epoch: 067, Loss: 0.42425, Train Acc: 0.86416, Val Acc: 0.84916, Best: 0.84916\n",
      "Epoch: 068, Loss: 0.42335, Train Acc: 0.86351, Val Acc: 0.84947, Best: 0.84947\n",
      "Epoch: 069, Loss: 0.42090, Train Acc: 0.86583, Val Acc: 0.84988, Best: 0.84988\n",
      "Epoch: 070, Loss: 0.41949, Train Acc: 0.86616, Val Acc: 0.84906, Best: 0.84988\n",
      "Epoch: 071, Loss: 0.41683, Train Acc: 0.86631, Val Acc: 0.85100, Best: 0.85100\n",
      "Epoch: 072, Loss: 0.41939, Train Acc: 0.86678, Val Acc: 0.84916, Best: 0.85100\n",
      "Epoch: 073, Loss: 0.41743, Train Acc: 0.86863, Val Acc: 0.85193, Best: 0.85193\n",
      "Epoch: 074, Loss: 0.41432, Train Acc: 0.86681, Val Acc: 0.84988, Best: 0.85193\n",
      "Epoch: 075, Loss: 0.41406, Train Acc: 0.86801, Val Acc: 0.85059, Best: 0.85193\n",
      "Epoch: 076, Loss: 0.41312, Train Acc: 0.86971, Val Acc: 0.85213, Best: 0.85213\n",
      "Epoch: 077, Loss: 0.41265, Train Acc: 0.86994, Val Acc: 0.85265, Best: 0.85265\n",
      "Epoch: 078, Loss: 0.41216, Train Acc: 0.87017, Val Acc: 0.85213, Best: 0.85265\n",
      "Epoch: 079, Loss: 0.40852, Train Acc: 0.87030, Val Acc: 0.85121, Best: 0.85265\n",
      "Epoch: 080, Loss: 0.40850, Train Acc: 0.87011, Val Acc: 0.85039, Best: 0.85265\n",
      "Epoch: 081, Loss: 0.40619, Train Acc: 0.87195, Val Acc: 0.85295, Best: 0.85295\n",
      "Epoch: 082, Loss: 0.40591, Train Acc: 0.87266, Val Acc: 0.85213, Best: 0.85295\n",
      "Epoch: 083, Loss: 0.40437, Train Acc: 0.87224, Val Acc: 0.85183, Best: 0.85295\n",
      "Epoch: 084, Loss: 0.40268, Train Acc: 0.87111, Val Acc: 0.85224, Best: 0.85295\n",
      "Epoch: 085, Loss: 0.40222, Train Acc: 0.87235, Val Acc: 0.85326, Best: 0.85326\n",
      "Epoch: 086, Loss: 0.40034, Train Acc: 0.87340, Val Acc: 0.85408, Best: 0.85408\n",
      "Epoch: 087, Loss: 0.39954, Train Acc: 0.87299, Val Acc: 0.85121, Best: 0.85408\n",
      "Epoch: 088, Loss: 0.39930, Train Acc: 0.87500, Val Acc: 0.85295, Best: 0.85408\n",
      "Epoch: 089, Loss: 0.39886, Train Acc: 0.87586, Val Acc: 0.85449, Best: 0.85449\n",
      "Epoch: 090, Loss: 0.39763, Train Acc: 0.87311, Val Acc: 0.84926, Best: 0.85449\n",
      "Epoch: 091, Loss: 0.39668, Train Acc: 0.87412, Val Acc: 0.85295, Best: 0.85449\n",
      "Epoch: 092, Loss: 0.39508, Train Acc: 0.87224, Val Acc: 0.85162, Best: 0.85449\n",
      "Epoch: 093, Loss: 0.39274, Train Acc: 0.87679, Val Acc: 0.85357, Best: 0.85449\n",
      "Epoch: 094, Loss: 0.39218, Train Acc: 0.87667, Val Acc: 0.85511, Best: 0.85511\n",
      "Epoch: 095, Loss: 0.39277, Train Acc: 0.87744, Val Acc: 0.85357, Best: 0.85511\n",
      "Epoch: 096, Loss: 0.39104, Train Acc: 0.87857, Val Acc: 0.85459, Best: 0.85511\n",
      "Epoch: 097, Loss: 0.38919, Train Acc: 0.87556, Val Acc: 0.85316, Best: 0.85511\n",
      "Epoch: 098, Loss: 0.38934, Train Acc: 0.87694, Val Acc: 0.85449, Best: 0.85511\n",
      "Epoch: 099, Loss: 0.38701, Train Acc: 0.87923, Val Acc: 0.85429, Best: 0.85511\n",
      "Epoch: 100, Loss: 0.38732, Train Acc: 0.88032, Val Acc: 0.85695, Best: 0.85695\n",
      "Epoch: 101, Loss: 0.38675, Train Acc: 0.87926, Val Acc: 0.85644, Best: 0.85695\n",
      "Epoch: 102, Loss: 0.38623, Train Acc: 0.87956, Val Acc: 0.85541, Best: 0.85695\n",
      "Epoch: 103, Loss: 0.38318, Train Acc: 0.88184, Val Acc: 0.85695, Best: 0.85695\n",
      "Epoch: 104, Loss: 0.38352, Train Acc: 0.88047, Val Acc: 0.85634, Best: 0.85695\n",
      "Epoch: 105, Loss: 0.38231, Train Acc: 0.87990, Val Acc: 0.85675, Best: 0.85695\n",
      "loss = 0.47301769256591797\r"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "for epoch in range(13, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    val_acc = test(val_loader)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"../data/play.pt\")\n",
    "        \n",
    "    \n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Val Acc: {:.5f}, Best: {:.5f}'.\n",
    "          format(epoch, loss, train_acc, val_acc, best_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 001, Loss: 1.28519, Train Acc: 0.64447, Val Acc: 0.64459, Best: 0.64459  \n",
    "Epoch: 002, Loss: 0.88842, Train Acc: 0.68974, Val Acc: 0.69206, Best: 0.69206  \n",
    "Epoch: 003, Loss: 0.79654, Train Acc: 0.71359, Val Acc: 0.71883, Best: 0.71883  \n",
    "Epoch: 004, Loss: 0.74853, Train Acc: 0.72913, Val Acc: 0.73164, Best: 0.73164  \n",
    "Epoch: 005, Loss: 0.71533, Train Acc: 0.74874, Val Acc: 0.75154, Best: 0.75154  \n",
    "Epoch: 006, Loss: 0.68941, Train Acc: 0.75900, Val Acc: 0.76138, Best: 0.76138  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
